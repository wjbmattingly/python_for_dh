{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "613bae22-5138-407f-85c3-89e00a8d7ab3",
   "metadata": {},
   "source": [
    "# Transformer Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfb309d-19b5-4821-af3d-eb3865980057",
   "metadata": {},
   "source": [
    "Transformer models are robust machine learning language models that are capable of solving many complex problems. It is beyond the scope of this textbook to explain the architecture of transformers or how they work precisely. Nevertheless, it is important to understand a few things. First, transformer models are particularly suited for working with multi-lingual documents. Second, they are powerful, but slow. Third, the way they represent texts is different from other language models in that they are able to change the vector, or numerical representation, of an individual word based on context. \n",
    "\n",
    "This makes them particularly suited to the problem of topic modeling because they can convert each document into a numerical representation that is not a sequence of numbers that correspond to the presence of an individual word. Instead, transformer models can convert a document into a deeply semantic representation that retains the context and syntax.\n",
    "\n",
    "In order to leverage transformer models, we need to install SentenceTransformers (from HuggingFace). We can do that with pip via the command below:\n",
    "\n",
    "```\n",
    "pip install sentence_transformers\n",
    "```\n",
    "\n",
    "In this section, we will also be working with two other libraries: umap (for representing our complex numerical representation of documents in 2-dimensional space) and hdbscan (for finding clusters). UMAP has gained popularity in recent years as a quick, effectively, and fairly accurate way to represent higher dimensional data in lower dimensions. In Python, we can access the UMAP algorithm through the UMAP library which can be installed with pip by typing the following command:\n",
    "\n",
    "`pip install umap-learn`\n",
    "\n",
    "Note the `-learn` after `umap`. This is very important as `umap` is an entirely different library.\n",
    "\n",
    "```\n",
    "pip install hdbscan\n",
    "```\n",
    "\n",
    "Now that our libraries are installed, we can begin importing them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27d8b7e-7551-4d1d-a826-006e22c84c79",
   "metadata": {},
   "source": [
    "## Importing Libraries and Gathering Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "720c9014-f185-4ac0-ac7c-52d58b0d5b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wjbmattingly/anaconda3/envs/python-textbook/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import umap\n",
    "import hdbscan\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060a936f-5990-4c84-90ab-6a52519fcb35",
   "metadata": {},
   "source": [
    "Before we begin leveraging advanced transformer-based topic modeling libraries, like Top2Vec, we should have a good understanding about how they work. Top2Vec leverages three libraries: Sentence Transformers, UMAP, and HDBScan. Here, we will explore each of these so that the reader will have a basic understanding of the theory and methods behind Top2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90d9c08c-9da5-4d4c-a5b9-95e3aecdcec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Last</th>\n",
       "      <th>First</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AARON</td>\n",
       "      <td>Thabo Simon</td>\n",
       "      <td>An ANCYL member who was shot and severely inju...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Last        First                                        Description\n",
       "0  AARON  Thabo Simon  An ANCYL member who was shot and severely inju..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/trc.csv\")\n",
    "df = df[[\"Last\", \"First\", \"Description\"]]\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15a9a627-d6a7-42a8-812c-2cf97c4b3b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = df.Description.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be32817-af93-4868-b9f1-836e127b0ad1",
   "metadata": {},
   "source": [
    "## Embedding the Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0215006e-f872-41ff-b54d-eea4dff8a639",
   "metadata": {},
   "source": [
    "The first step in using transformers in topic modeling is to convert the text into a vector. We met vectors when we explored LDA topic modeling in the previous chapter. Arrays for LDA topic modeling were rooted in a bag-of-word (BOW) index. This index, while computationally light, did not retain semantic meaning or word order.\n",
    "\n",
    "When we are working with transformers, we can create a vector for each document in our dataset. This vector is not an index of the words used, rather it is an embedding for the entire document that contains its semantic usages of words. It also preserves in this same vector space the word order to a degree. This document vector is similar to the word vector that we met in Part Three of this textbook. Instead of embedding a single word, however, the entire document receives an embedding. This allows us to mathematically compare documents across an entire corpus.\n",
    "\n",
    "To convert our documents into vectors, we first need a transformer model. Fortunately, the Sentence Transformer library from HuggingFace allows us to easily load robust pre-trained language models. In our case, we will be using the `all-MiniLM-L6-v2` model. We can load this model by calling the Sentence Transformer class from the sentence_transformers library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4464e80d-ecff-42d1-a890-7c30371d2baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7f29d6-ffc0-4750-b824-a39ab9f5120c",
   "metadata": {},
   "source": [
    "Once our model class is created, we can use the `.encode()` method. This method will encode all the documents that we pass to it. In our case, this is the approximately 22,000 descriptions from the TRC dataset. The `.encode()` method takes a single mandatory argument, a list of data to embed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "732055a3-ce5e-4486-81ef-ec4d61492456",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_embeddings = model.encode(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86aebb04-0f5c-4c3b-81e6-9dfa65cbfdcc",
   "metadata": {},
   "source": [
    "Now that we have the vectors for each document, let's examine one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "923ce7b0-9c1a-4a05-84f7-a1a7e4ea3c52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.07123438,  0.00332599, -0.05571468,  0.08363082,  0.09066874,\n",
       "        0.05503598,  0.08029197,  0.01370712,  0.05912026,  0.06226278],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_embeddings[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dde0a6-bea0-4243-aca6-1350562a22e5",
   "metadata": {},
   "source": [
    "As we can see, this looks remarkably similar to our word embeddings. While this is useful for examining mathematically comparing the similarity between documents, it can be difficult to parse this numerical data visually. For this reason, it is useful to flatten the data into 2 or 3 dimensions. This allows the data to be graphed. In the previous chapter, we learned how to flatten data with PCA. In this chapter, we will meet a new dimensionality reduction algorithm, UMAP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f181f598-e80b-41c1-830f-f5a7360b64db",
   "metadata": {},
   "source": [
    "## Flattening the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa219641-6b20-4c14-b897-0c2c05590ae3",
   "metadata": {},
   "source": [
    "Once you have installed UMAP correctly, you can access the `UMAP` class. This will take several parameters that can be adjusted to yield different results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7bb3dea-1752-4dfd-8c04-6ea3b797f49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_proj = umap.UMAP(n_neighbors=10,\n",
    "                          min_dist=0.01,\n",
    "                          metric='correlation').fit_transform(doc_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39809f8d-95e8-4a52-9f48-81a27e3be5c5",
   "metadata": {},
   "source": [
    "## Isolating Clusters with HDBScan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73657b7a-c68c-40d0-b3a6-20f0d13b99d3",
   "metadata": {},
   "source": [
    "Once our data has been flattened, we can automatically identify the number of clusters within it and assign documents to each cluster with the HDBScan algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d7e2a74-ffb2-434e-8898-73be928a8d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2317\n"
     ]
    }
   ],
   "source": [
    "hdbscan_labels = hdbscan.HDBSCAN(min_samples=2, min_cluster_size=3).fit_predict(umap_proj)\n",
    "print(len(set(hdbscan_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63034bbd-f454-4a62-a3a9-6b50620dee98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Last</th>\n",
       "      <th>First</th>\n",
       "      <th>Description</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AARON</td>\n",
       "      <td>Thabo Simon</td>\n",
       "      <td>An ANCYL member who was shot and severely inju...</td>\n",
       "      <td>7.330942</td>\n",
       "      <td>-0.935302</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Last        First                                        Description  \\\n",
       "0  AARON  Thabo Simon  An ANCYL member who was shot and severely inju...   \n",
       "\n",
       "          x         y  topic  \n",
       "0  7.330942 -0.935302     -1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"x\"] = umap_proj[:, 0]\n",
    "df[\"y\"] = umap_proj[:, 1]\n",
    "df[\"topic\"] = hdbscan_labels\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a95eb1-fb3d-400b-bde0-638c01c89816",
   "metadata": {},
   "source": [
    "## Analyzing the Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58c16b7-6109-4fcd-83fa-8289c55ec6ee",
   "metadata": {},
   "source": [
    "Now that we have the labels loaded into our DataFrame, we can use Pandas to interrogate that data. Let's grab a topic and examine it. Here, we will examine topic 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fbda2a58-6608-4283-ba2f-80b914c6b4a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Was one of thirteen people killed in an attack by UDF and ANC supporters on Inkatha supporters in the Mahwaqa area, near Port Shepstone, Natal, on 24 March 1990. Two UDF supporters were granted amnesty (AC/2000/041).\n",
      "\n",
      "An ANC supporter who was shot dead on 6 April 1990 when a group of Inkatha supporters attacked UDF supporters and residents at Mpumalanga, KwaZulu, near Durban, in spite of a heavy police and military presence. Fourteen people were killed and at least one hundred and twenty homes were burnt down. One former IFP member was granted amnesty (AC/1999/0332).\n",
      "\n",
      "An ANC supporter who was shot and killed when a group of Inkatha supporters and Caprivi trainees attacked a UDF meeting in a house at Mpumalanga, KwaZulu, near Durban, on 18 January 1988. Nine people were killed and an estimated two hundred people were injured in the attack. The group went on to destroy around eight houses. One former Inkatha member was granted amnesty (AC/1999/0332).\n",
      "\n",
      "An ANC supporter who was shot and killed when a group of Inkatha supporters and Caprivi trainees attacked a UDF meeting in a house at Mpumalanga, KwaZulu, near Durban, on 18 January 1988. Nine people were killed and an estimated two hundred people were injured in the attack. The group went on to destroy around eight houses. One former Inkatha member was granted amnesty (AC/1999/0332).\n",
      "\n",
      "An ANC supporter who was shot dead on 18 January 1988 when a group of Inkatha supporters, including some Caprivi trainees, opened fire on a UDF meeting in a house at Mpumalanga, KwaZulu, near Durban. Nine people were killed and an estimated two hundred people were injured. The group went on to destroy about eight houses. Amnesty applications were received for this incident.\n",
      "\n",
      "Was one of thirteen people killed in an attack by UDF and ANC supporters on Inkatha supporters in the Mahwaqa area, near Port Shepstone, Natal, on 24 March 1990. Two UDF supporters were granted amnesty (AC/2000/041).\n",
      "\n",
      "Was one of thirteen people killed in an attack by UDF and ANC supporters on Inkatha supporters in the Mahwaqa area, near Port Shepstone, Natal, on 24 March 1990. Two UDF supporters were granted amnesty (AC/2000/041).\n",
      "\n",
      "Was abducted, interrogated and stabbed to death by UDF and ANC supporters at Mpumalanga, Natal, in July 1989. He was believed to be an Inkatha member. One UDF and ANC supporter was granted amnesty (AC/200/011).\n",
      "\n",
      "Was one of thirteen people killed in an attack by UDF and ANC supporters on Inkatha supporters in the Mahwaqa area, near Port Shepstone, Natal, on 24 March 1990. Two UDF supporters were granted amnesty (AC/2000/041).\n",
      "\n",
      "Was one of thirteen people killed in an attack by UDF and ANC supporters on Inkatha supporters in the Mahwaqa area, near Port Shepstone, Natal, on 24 March 1990. Two UDF supporters were granted amnesty (AC/2000/041).\n",
      "\n",
      "An Inkatha supporter who was stabbed and severely injured by named ANC supporters in Mtwalume, near Umzinto, Natal, in political conflict in the area on 4 February 1990 following the unbanning of political organisations two days earlier. Two UDF supporters were granted amnesty for the attack (AC/2000/041).\n",
      "\n",
      "Was one of thirteen people killed in an attack by UDF and ANC supporters on Inkatha supporters in the Mahwaqa area, near Port Shepstone, Natal, on 24 March 1990. Two UDF supporters were granted amnesty (AC/2000/041). \n",
      "\n",
      "Was one of thirteen people killed in an attack by UDF and ANC supporters on Inkatha supporters in the Mahwaqa area, near Port Shepstone, Natal, on 24 March 1990. Two UDF supporters were granted amnesty (AC/2000/041). \n",
      "\n",
      "Was one of thirteen people killed in an attack by UDF and ANC supporters on Inkatha supporters in the Mahwaqa area, near Port Shepstone, Natal, on 24 March 1990. Two UDF supporters were granted amnesty (AC/2000/041). \n",
      "\n",
      "Was one of thirteen people killed in an attack by UDF and ANC supporters on Inkatha supporters in the Mahwaqa area, near Port Shepstone, Natal, on 24 March 1990. Two UDF supporters were granted amnesty (AC/2000/041). \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for d in df.loc[df.topic == 100].Description.tolist():\n",
    "    print(d)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99a3e04-6fd6-4b3d-af1d-012fe705101f",
   "metadata": {},
   "source": [
    "Notice that these results for `Topic 100` all clearly have overlapping similarity. In some cases, these are identical, in others they are quite similar.  Nevertheless, we have one major limitation to this approach, outliers or noise. Let's examine `Topic -1`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b208aeea-2dac-4621-bdf3-0d646e2a4c05",
   "metadata": {},
   "source": [
    "## Outliers (Noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30df494a-9c08-4b27-a2a4-d963d98c287b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Last</th>\n",
       "      <th>First</th>\n",
       "      <th>Description</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AARON</td>\n",
       "      <td>Thabo Simon</td>\n",
       "      <td>An ANCYL member who was shot and severely inju...</td>\n",
       "      <td>7.330942</td>\n",
       "      <td>-0.935302</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ABRAHAMS</td>\n",
       "      <td>Moegsien</td>\n",
       "      <td>Was stabbed and stoned to death by a group of ...</td>\n",
       "      <td>5.861767</td>\n",
       "      <td>10.907988</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ADAMS</td>\n",
       "      <td>Zwelinzima Sidwell</td>\n",
       "      <td>Was severely beaten and shot in the leg in Gug...</td>\n",
       "      <td>5.548826</td>\n",
       "      <td>9.517207</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>AGGETT</td>\n",
       "      <td>Neil Hudson</td>\n",
       "      <td>Died in detention at John Vorster Square, Joha...</td>\n",
       "      <td>11.213459</td>\n",
       "      <td>8.340458</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>ALBERT</td>\n",
       "      <td>Nombuyiselo Francis</td>\n",
       "      <td>Was beaten and stabbed to death on 10 December...</td>\n",
       "      <td>6.867918</td>\n",
       "      <td>2.346848</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20804</th>\n",
       "      <td>XULU</td>\n",
       "      <td>Josephina</td>\n",
       "      <td>An IFP supporter who had three rondavels burnt...</td>\n",
       "      <td>2.483765</td>\n",
       "      <td>3.955176</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20810</th>\n",
       "      <td>XULU</td>\n",
       "      <td>Mzomonje Phineas</td>\n",
       "      <td>Was stabbed to death in Inchanga, Natal, on 15...</td>\n",
       "      <td>5.103362</td>\n",
       "      <td>11.622403</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20820</th>\n",
       "      <td>XULU</td>\n",
       "      <td>Sipho Aubrey</td>\n",
       "      <td>An ANC supporter who was shot and fatally woun...</td>\n",
       "      <td>7.560771</td>\n",
       "      <td>0.817540</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20821</th>\n",
       "      <td>XULU</td>\n",
       "      <td>Sipho Brigitte</td>\n",
       "      <td>An MK operative who was executed in Pretoria C...</td>\n",
       "      <td>3.011799</td>\n",
       "      <td>-0.359574</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20830</th>\n",
       "      <td>YAKA</td>\n",
       "      <td>Mbangomuni</td>\n",
       "      <td>An IFP supporter and acting induna who was sho...</td>\n",
       "      <td>8.195865</td>\n",
       "      <td>2.783923</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4579 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Last                First  \\\n",
       "0         AARON          Thabo Simon   \n",
       "7      ABRAHAMS             Moegsien   \n",
       "16        ADAMS   Zwelinzima Sidwell   \n",
       "31       AGGETT          Neil Hudson   \n",
       "34       ALBERT  Nombuyiselo Francis   \n",
       "...         ...                  ...   \n",
       "20804      XULU            Josephina   \n",
       "20810      XULU     Mzomonje Phineas   \n",
       "20820      XULU         Sipho Aubrey   \n",
       "20821      XULU       Sipho Brigitte   \n",
       "20830      YAKA           Mbangomuni   \n",
       "\n",
       "                                             Description          x  \\\n",
       "0      An ANCYL member who was shot and severely inju...   7.330942   \n",
       "7      Was stabbed and stoned to death by a group of ...   5.861767   \n",
       "16     Was severely beaten and shot in the leg in Gug...   5.548826   \n",
       "31     Died in detention at John Vorster Square, Joha...  11.213459   \n",
       "34     Was beaten and stabbed to death on 10 December...   6.867918   \n",
       "...                                                  ...        ...   \n",
       "20804  An IFP supporter who had three rondavels burnt...   2.483765   \n",
       "20810  Was stabbed to death in Inchanga, Natal, on 15...   5.103362   \n",
       "20820  An ANC supporter who was shot and fatally woun...   7.560771   \n",
       "20821  An MK operative who was executed in Pretoria C...   3.011799   \n",
       "20830  An IFP supporter and acting induna who was sho...   8.195865   \n",
       "\n",
       "               y  topic  \n",
       "0      -0.935302     -1  \n",
       "7      10.907988     -1  \n",
       "16      9.517207     -1  \n",
       "31      8.340458     -1  \n",
       "34      2.346848     -1  \n",
       "...          ...    ...  \n",
       "20804   3.955176     -1  \n",
       "20810  11.622403     -1  \n",
       "20820   0.817540     -1  \n",
       "20821  -0.359574     -1  \n",
       "20830   2.783923     -1  \n",
       "\n",
       "[4579 rows x 6 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df.topic == -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bb1d88-b77f-4a56-940a-e9899b184372",
   "metadata": {},
   "source": [
    "We have 4,579 outliers, or documents that do not neatly cluster into any one category. This is a clear limitation of this approach. What do we do with these outliers? One option is to assign them to the nearest topic. The process of doing this, however, can be quite complex as the we calculate distance between a document vector and a topic can vary depending on which measurements we wish to use. In the final section of this chapter, we will examine a library that handles this problem for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072967b6-14b6-4bff-8cde-d9dc7ecbb264",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
